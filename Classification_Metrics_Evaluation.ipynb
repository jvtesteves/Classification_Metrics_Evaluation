{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAq6xw0Phgfeb+xkKaDgF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvtesteves/Classification_Metrics_Evaluation/blob/main/Classification_Metrics_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Metrics Evaluation Notebook\n",
        "\n",
        "This notebook demonstrates how to calculate the key evaluation metrics for classification models, including Accuracy, Sensitivity (Recall), Specificity, Precision, and F-score.\n",
        "\n",
        "In classification tasks, these metrics are derived from the confusion matrix. A confusion matrix is used to summarize the performance of a model by comparing the actual and predicted labels.\n"
      ],
      "metadata": {
        "id": "ykgbUTsVyFsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table with four outcomes that help to evaluate the performance of a classification model:\n",
        "\n",
        "- **TP (True Positives)**: The number of positive instances correctly predicted as positive.\n",
        "- **TN (True Negatives)**: The number of negative instances correctly predicted as negative.\n",
        "- **FP (False Positives)**: The number of negative instances incorrectly predicted as positive.\n",
        "- **FN (False Negatives)**: The number of positive instances incorrectly predicted as negative.\n",
        "\n",
        "For this example, we will assume the following arbitrary values:\n",
        "- TP = 50\n",
        "- TN = 40\n",
        "- FP = 10\n",
        "- FN = 5\n"
      ],
      "metadata": {
        "id": "zwyWl5RbyIBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Using the confusion matrix, the metrics are calculated as follows:\n",
        "\n",
        "- **Accuracy**: Measures the overall correctness of the model.\n",
        "  \n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "  $$\n",
        "  \n",
        "- **Sensitivity (Recall)**: Measures the proportion of actual positives correctly identified.\n",
        "  \n",
        "  $$\n",
        "  \\text{Sensitivity} = \\frac{TP}{TP + FN}\n",
        "  $$\n",
        "  \n",
        "- **Specificity**: Measures the proportion of actual negatives correctly identified.\n",
        "  \n",
        "  $$\n",
        "  \\text{Specificity} = \\frac{TN}{TN + FP}\n",
        "  $$\n",
        "  \n",
        "- **Precision**: Measures the proportion of predicted positives that are actually positive.\n",
        "  \n",
        "  $$\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  $$\n",
        "  \n",
        "- **F-score (F1-score)**: The harmonic mean of Precision and Sensitivity, providing a balance between the two.\n",
        "  \n",
        "  $$\n",
        "  \\text{F-score} = \\frac{2 \\times \\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}}\n",
        "  $$\n"
      ],
      "metadata": {
        "id": "MOFm7BHQyNQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define confusion matrix values\n",
        "TP = 50  # True Positives\n",
        "TN = 40  # True Negatives\n",
        "FP = 10  # False Positives\n",
        "FN = 5   # False Negatives\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "specificity = TN / (TN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "f_score = (2 * precision * sensitivity) / (precision + sensitivity)\n",
        "\n",
        "# Print the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"TP (True Positives): {TP}\")\n",
        "print(f\"TN (True Negatives): {TN}\")\n",
        "print(f\"FP (False Positives): {FP}\")\n",
        "print(f\"FN (False Negatives): {FN}\\n\")\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
        "print(f\"Specificity: {specificity:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"F-score: {f_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "913hbYBuyOFP",
        "outputId": "a47958fb-5b14-4f2b-a8c4-6954a8eb7b55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "TP (True Positives): 50\n",
            "TN (True Negatives): 40\n",
            "FP (False Positives): 10\n",
            "FN (False Negatives): 5\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.86\n",
            "Sensitivity (Recall): 0.91\n",
            "Specificity: 0.80\n",
            "Precision: 0.83\n",
            "F-score: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion of Results\n",
        "\n",
        "The calculations above show how the values from the confusion matrix can be used to derive meaningful metrics:\n",
        "\n",
        "- **Accuracy** indicates the overall proportion of correct predictions.\n",
        "- **Sensitivity (Recall)** is crucial when the cost of missing positive cases is high.\n",
        "- **Specificity** is important when it is essential to correctly identify negative cases.\n",
        "- **Precision** tells us how reliable the positive predictions are.\n",
        "- **F-score** provides a single metric that balances Precision and Sensitivity, especially useful when you need to account for both false positives and false negatives.\n",
        "\n",
        "These metrics are fundamental in assessing the performance of classification models.\n"
      ],
      "metadata": {
        "id": "Rs8UcyEtydyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has provided a step-by-step approach to calculating the key metrics for evaluating classification models. By adjusting the confusion matrix values, you can simulate different scenarios and better understand how these metrics change with model performance.\n",
        "\n",
        "Feel free to modify the code to evaluate your own models or datasets.\n"
      ],
      "metadata": {
        "id": "2BFAy_PAyega"
      }
    }
  ]
}